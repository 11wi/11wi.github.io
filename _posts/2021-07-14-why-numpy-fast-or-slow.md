ì¶”ì²œ ëª¨ë¸ ì—°ì‚°ì— í–‰ë ¬ ê³±ì…ˆì´ ë§ì´ ì“°ì´ëŠ”ë° ì°¸ ì‹ ê¸°í•˜ê²Œë„ ì‚¬ì†Œí•œ ì½”ë“œì—ë„ ì„±ëŠ¥ì´ ë‹¬ë¼ì§„ë‹¤.
ì •í™•í•œ ì›ë¦¬ë¥¼ ì•Œê³  ì‹œìŠ¤í…œì„ ê°œì„ í•˜ê³ ì 2ê°œì›”ì— ê±¸ì³ í‹ˆí‹ˆíˆ ì •ë¦¬í•´ë†“ì•˜ë‹¤.
ë³´í†µ ML Engineerë¼ë©´ ì´ëŸ¬í•œ ì„±ëŠ¥ optimizeëŠ” ê´€ì‹¬ì´ ìˆì„í…ë° ìƒê°ë³´ë‹¤ ì˜ ì •ë¦¬ëœ ê¸€ì„ ëª» ë´ì„œ ì§ì ‘ ë§Œë“¤ì–´ë²„ë ¸ë‹¤ ğŸ˜ 

---

ì›ë˜ numeric computationì— ì í•©í•œ ì–¸ì–´ë¡œ C, C++, Fortran(ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ìŒ)ì´ ê¼½í˜”ê³  Pythonì€ ì í•©í•œ ì–¸ì–´ê°€ ì•„ë‹ˆì—ˆë‹¤. ì´ë¥¼ ë³´ì™„í•˜ê³ ì 2005ë…„ numpyê°€ ê³µê°œë˜ì—ˆë‹¤.

numpyëŠ” scipyì™€ë„ ê´€ê³„ê°€ ê¹Šì€ë° scipyëŠ” [ê²€ì¦ëœ scientific packageë¥¼](http://www.netlib.org) wrappingí•œ ê²ƒì´ê³ , ì´ì— ì‚¬ìš©ë˜ëŠ” numeric objectë¥¼ numpyì—ì„œ ì œê³µí•œë‹¤. <https://www.scipy.org/scipylib/faq.html#how-can-scipy-be-fast-if-it-is-written-in-an-interpreted-language-like-python>

numpyì˜ í•µì‹¬ì€ ndarray(n-dimensional array) classì™€ numeric operationì„ Cython ë° BLASë¡œ êµ¬í˜„í•œ ê²ƒì´ë‹¤. ndarrayëŠ” fixed size ê°ì²´ì´ë‹¤. ë”°ë¼ì„œ append ë™ì‘ì€ new size copyë¥¼ ìƒì„±í•˜ê³  ì´ì „ sizeë¥¼ ì‚­ì œí•œë‹¤. <https://scipy-lectures.org/advanced/advanced_numpy/>

array ì €ì¥ ë°©ì‹ì€ C order, F orderê°€ ìˆëŠ”ë° ê°ê° Cì™€ Fortranì˜ êµ¬í˜„ì²´ì™€ ëŒ€ì‘ëœë‹¤. CëŠ” row-wise, FëŠ” column-wiseë¡œ strideì˜ ì°¨ì´ê°€ ìˆë‹¤. tobytes()ë¡œ í™•ì¸í•´ë³´ë©´ 1ì°¨ì› arrayì˜ indexë¡œ í–‰/ì—´ì„ êµ¬ë¶„í•¨ì„ ì•Œ ìˆ˜ ìˆë‹¤.

```
c = np.array([[1, 2, 3],
              [4, 5, 6],
              [7, 8, 9]], dtype=np.int16, order='C')
assert c.strides == (6, 2)

f = np.array(x, order='F')
assert f.strides == (2, 6)

assert f.T.data.f_contiguous, 'transpose changed strides and c order'
```

---

## LAPACKê³¼ BLAS

LAPACKì€ Linear Algebra PACKageìœ¼ë¡œ fortranìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë©° linear system algorithmì„ ì œê³µí•œë‹¤. LAPACKì˜ ë‚´ë¶€ëŠ” ëŒ€ë¶€ë¶„ BLAS í˜¸ì¶œë¡œ ì´ë¤„ì ¸ìˆë‹¤. hardware accelerationìœ¼ë¡œ ë¶„ë¥˜ëœë‹¤.

BLASëŠ” Strassen algorithmì²˜ëŸ¼ ë³µì¡í•œ codeë¥¼ ì‚¬ìš©í•œ ê²ƒì´ ì•„ë‹ˆë¼ cpu cache optimizeì— ëŒ€í•œ ê²ƒì´ë‹¤. <https://stackoverflow.com/questions/1303182/how-does-blas-get-such-extreme-performance> SIMD(single instruction multiple data) ê°œë…ì„ êµ¬í˜„í•œ SSE(Streaming SIMD Extensions), AVX(Advanced Vector Extensions) ê¸°ìˆ ì„ ì‚¬ìš©í•œë‹¤. C codeì—ì„œ ì´ë¥¼ í˜¸ì¶œí•˜ë©´ `#include <emmintrin.h> _mm_mul_pd`ì™€ ê°™ì€ íŠ¹ìˆ˜í•œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤. <http://www.mathematik.uni-ulm.de/~lehn/apfel/sghpc/gemm/> <https://software.intel.com/sites/landingpage/IntrinsicsGuide/#>
í•˜ì§€ë§Œ ìµœê·¼ GPUë¥¼ ì£¼ë¡œ ì‚¬ìš©í•˜ë©´ì„œ AVXì™€ ê°™ì€ CPU ìµœì í™” ê¸°ìˆ ì€ ì„±ì¥ë‘”í™”ë  ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.

---

matrix multiplicationì˜ ì‹œê°„ ë³µì¡ë„ëŠ” n^3ì´ì§€ë§Œ [ìŠˆíŠ¸ë¼ì„¼ ì•Œê³ ë¦¬ì¦˜](https://en.wikipedia.org/wiki/Computational_complexity_of_matrix_multiplication#Strassen's_algorithm)ì€ n^2.807ì´ë‹¤.
numpyëŠ” [blasë¥¼ ì‚¬ìš©í•˜ë©°](https://github.com/numpy/numpy/blob/v1.20.3/numpy/core/src/multiarray/arraytypes.c.src#L3539) multithreadë¡œ ì‹¤í–‰ëœë‹¤. 
blasëŠ” mkl, openblasê°€ ì„±ëŠ¥ì´ ì¢‹ë‹¤. <https://software.intel.com/content/www/us/en/develop/articles/numpyscipy-with-intel-mkl.html>
scipy apië¡œ blasë¥¼ ì§ì ‘ í˜¸ì¶œí•  ìˆ˜ ìˆë‹¤.

```
>>> import numpy as np
>>> np.show_config()
blas_mkl_info:
  NOT AVAILABLE
blis_info:
  NOT AVAILABLE
openblas_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
blas_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
lapack_mkl_info:
  NOT AVAILABLE
openblas_lapack_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
lapack_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
```

ë³„ë„ ì„¤ì • ì—†ëŠ” m1 macì—ì„œ openblas linkageë¥¼ í™•ì¸í–ˆë‹¤. intel cpuê°€ ì•„ë‹ˆê¸° ë•Œë¬¸ì— ë‹¹ì—°íˆ mklì€ ì•ˆëœë‹¤.

---

blas ì§ì ‘ í˜¸ì¶œì‹œì˜ ì„±ëŠ¥

```
import numpy as np
from scipy.linalg.blas import dgemm, sgemm

a = np.random.rand(10000, 10000)

%timeit a@a
%timeit dgemm(1, a, a)
%timeit dgemm(1, a, a, trans_a=1, trans_b=1)
```

---

í™˜ê²½ë³€ìˆ˜ë¥¼ í†µí•´ multithread ê°œìˆ˜ë¥¼ ì§€ì •í•  ìˆ˜ ìˆë‹¤.

```
OMP_NUM_THREADS: openmp,
OPENBLAS_NUM_THREADS: openblas,
MKL_NUM_THREADS: mkl,
VECLIB_MAXIMUM_THREADS: accelerate,
NUMEXPR_NUM_THREADS: numexpr
```

```
import os
os.environ["OMP_NUM_THREADS"] = "4" # export OMP_NUM_THREADS=4
os.environ["OPENBLAS_NUM_THREADS"] = "4" # export OPENBLAS_NUM_THREADS=4 
os.environ["MKL_NUM_THREADS"] = "6" # export MKL_NUM_THREADS=6
os.environ["VECLIB_MAXIMUM_THREADS"] = "4" # export VECLIB_MAXIMUM_THREADS=4
os.environ["NUMEXPR_NUM_THREADS"] = "6" # export NUMEXPR_NUM_THREADS=6
```

ì„±ëŠ¥ ì‹¤í—˜ì„ í•´ë³´ì.

```
import os
os.environ["OPENBLAS_NUM_THREADS"] = "4"
import numpy as np
P = np.random.rand(10000, 500)
Q = np.random.rand(10000, 500)
%timeit P @ Q.T
```

```
1.78 s Â± 161 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
```

```
import os
os.environ["OPENBLAS_NUM_THREADS"] = "1"
import numpy as np
P = np.random.rand(10000, 500)
Q = np.random.rand(10000, 500)
%timeit P @ Q.T
```

```
6.95 s Â± 245 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
```

threadì— ë”°ë¥¸ ì„±ëŠ¥ ì°¨ì´ê°€ í™•ì—°í•˜ë‹¤.


---

dtypeì„ ê¸°ë³¸ì ìœ¼ë¡œ np.float64(double)ë¡œ ì„¤ì •ë˜ëŠ”ë° ì´ë¥¼ np.float32(single)ë¡œ ë³€ê²½ë§Œ í•´ë„ ì„±ëŠ¥ í–¥ìƒì´ ìˆë‹¤.


```
import numpy as np
P = np.random.rand(10000, 500)
Q = np.random.rand(10000, 500)
P, Q = np.asarray(P, dtype=np.float32), np.asarray(Q, dtype=np.float32)
%timeit P @ Q.T

237 ms Â± 24.4 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
```

---

strideê°€ ì‘ìœ¼ë©´ cpuì—ì„œ fetchë¥¼ ì ê²Œ í•  ìˆ˜ ìˆì–´ ì„±ëŠ¥ì´ ì¢‹ë‹¤.

```
x = np.zeros((20000,))
y = np.zeros((20000*67,))[::67]
print(x.strides, y.strides)
# (8,) (536,)
%timeit x.sum()
%timeit y.sum()

6.83 Âµs Â± 86.5 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
34.6 Âµs Â± 1.59 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
```

---

numpy codeë¥¼ ì–´ë–»ê²Œ ì§œëŠëƒì— ë”°ë¼ ì†ë„ ì°¨ì´ê°€ í°ë°, ì´ëŠ” temporary arrayë¥¼ ë§Œë“¤ë©´ì„œ cache ì‚¬ìš©ì„ ì œëŒ€ë¡œ í•˜ì§€ ëª»í•˜ê¸° ë•Œë¬¸ì— ë°œìƒí•œë‹¤.

cache ì‚¬ìš©ì— ë”°ë¼ í†µìƒ 2~4ë°° ì„±ëŠ¥ ì°¨ì´ê°€ ìˆë‹¤. <https://numexpr.readthedocs.io/projects/NumExpr3/en/latest/intro.html#how-it-works>

cache ì „ëµì€ cache sizeë§Œí¼ chunkí•˜ê¸°, ì—°ì‚°í•˜ëŠ” dataì˜ addressë¥¼ ê°€ê¹ê²Œ í•˜ê¸° (aligned data).

```
import numpy as np
P = np.random.rand(10000, 5000)
Q = np.random.rand(10000, 5000)
P, Q = np.asarray(P, dtype=np.float32), np.asarray(Q, dtype=np.float32)
import numexpr as ne
%timeit 3 * P + 2 * Q
%timeit ne.evaluate("3 * P + 2 * Q")

121 ms Â± 990 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
42.4 ms Â± 178 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
```

---

optimizationì„ ìœ„í•´ numpyë¥¼ numba, cythonìœ¼ë¡œ ë°”ê¾¸ê¸°ë„ í•œë‹¤.

ì„œë¡œ ì„±ëŠ¥ì€ ë¹„ìŠ·í•˜ë‚˜ numbaëŠ” numeric array ì—°ì‚°ë§Œ ì‰½ê³  ë‹¤ë¥¸ ê¸°ëŠ¥ì€ ì œí•œì´ ë§ìœ¼ë©° conda í™˜ê²½ì„ ê¶Œì¥í•œë‹¤. cythonì€ python codeë¥¼ ëŒ€ë¶€ë¶„ ì“¸ ìˆ˜ ìˆê³  robustí•˜ë‹¤. <http://stephanhoyer.com/2015/04/09/numba-vs-cython-how-to-choose/>

numpy + cythonì€ numpy C APIë¥¼ ì¨ì„œ í˜ë“¤ë‹¤ê³ ëŠ” í•œë‹¤. <https://cython.readthedocs.io/en/latest/src/tutorial/numpy.html>

í•˜ì§€ë§Œ ì‹¤ì§ˆì ìœ¼ë¡œ MKLê³¼ ê°™ì€ kernel libë¥¼ ë°°ì œí• ìˆœ ì—†ë‹¤. <https://stackoverflow.com/questions/56920713/numpy-faster-than-numba-and-cython-how-to-improve-numba-code>

---

distributed computingì€ OpenMPIë¡œ êµ¬í˜„ëœë‹¤. https://mpi4py.readthedocs.io/en/stable/tutorial.html [uber ë¶„ì‚° í•™ìŠµ ì‹œìŠ¤í…œ](https://github.com/horovod/horovod)ì—ì„œë„ ì‚¬ìš©í•œë‹¤.

openclì€ cudaì²˜ëŸ¼ gpu programming

---

## octave benchmark

```
[_,_,t,_]=speed("p*q'", 'p=rand(30000,500); q=rand(30000,500)');
mean(t)
ans =  2.7118
```

## numpy benchmark

```
import numpy as np
p = np.random.rand(30000, 500)
q = np.random.rand(30000, 500)
Pf, Qf = np.asarray(p, dtype=np.float32, order='F'), np.asarray(q, dtype=np.float32, order='F')
Pc, Qc = np.asarray(p, dtype=np.float32, order='C'), np.asarray(q, dtype=np.float32, order='C')
%timeit Pf @ Qf.T
%timeit Pc @ Qc.T

# macbook pro 16 inch 2019
2.71 s Â± 213 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
2.88 s Â± 19 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)

# m5.16xlarge
2.11 s Â± 827 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
1.82 s Â± 136 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
```

## materials

https://developer.ibm.com/blogs/use-python-for-scientific-research/
https://developer.ibm.com/languages/python/articles/ba-accelerate-python/
https://users.ece.cmu.edu/~franzf/papers/gttse07.pdf

## cython

https://cython.readthedocs.io/en/latest/src/userguide/parallelism.html
https://github.com/pydata/bottleneck/issues/92
https://github.com/benfred/implicit/blob/master/implicit/nearest_neighbours.h
https://numpy.org/doc/1.17/reference/c-api.html

